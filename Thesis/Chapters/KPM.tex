The Kernel Polynomial Method (KPM) is a powerful technique for approximating spectral properties of large matrices~\cite{weisse2006,linsaadyang14}. It employs polynomial expansions to efficiently estimate quantities such as the spectral density and related functions. We previously motivated the replacement of the Dirac delta distribution with a basis of regular functions~$f_k$. We will focus on Chebyshev polynomials for this purpose.

\section{Basic Properties of Chebyshev Polynomials}
We now develop an algorithm for the KPM that incorporates the regularization of the spectral density using Chebyshev polynomials of the first kind. As mentioned in the previous chapter, other polynomial bases could also be considered, which is why the KPM has several variants.

First, let's look how Chebyshev polynomials are defined. Using the trigonometric functions, they can be expressed as follows:

\[ T_k(x) =
\begin{cases}
\cos(k \arccos(x))                & \quad \text{for } k \in [-1, 1],\\
    \cosh(k \arcosh(x))           & \quad \text{for } k > 1,\\
    (-1)^k \cosh(k \arcosh(-x))   & \quad \text{for } k < -1.
\end{cases}
\]
For being able to make use of the polynomials' orthogonality, we only use the formula~$T_k(x) = \cos(k \arccos(x))$. This means to only consider matrices that have eigenvalues within the interval~$[-1, 1]$. In the case that this condition should not be fulfilled, the eigenvalues can be transformed accordingly.

For this, let~$\lambda_{lb}$ and~$\lambda_{ub}$ be the lower and upper bound for the eigenvalues of~$A$, respectively. To find these, well-established methods like the Gershgorin circle theorem can be used~\cite{hornjohnson2013}. Define
\[
c := \frac{\lambda_{lb} + \lambda_{ub}}{2} \quad \text{and} \quad d := \frac{\lambda_{ub} - \lambda_{lb}}{2}
\]
Then, the matrix~$B = \frac{A - c*I_n}{d}$ has eigenvalues in the interval~$[-1, 1]$. A visualization of this is linked in the bibliography.

Now, let
\[
h(x) := \frac{1}{\sqrt{1 - x^2}}
\]
be a weight function on~$[-1, 1]$. We investigate the inner product of two Chebyshev polynomials with respect to this weight function:

\begin{lemma}[Orthogonality of Chebyshev Polynomials]
Let~$T_k(x)$ denote the Chebyshev polynomials, and let~$h(x) = \frac{1}{\sqrt{1 - x^2}}$ be the weight function on~$[-1, 1]$. Then, for all integers~$k, l \geq 0$, we have that
\begin{equation} \label{eq:orthogonality}
    \left \langle T_k, T_l \right \rangle_h = \int\limits_{-1}^1 \frac{1}{\sqrt{1 - x^2}} T_k(x) T_l(x) \dx =
    \begin{cases}
        0               & \text{if } k \neq l,\\
        \pi             & \text{if } k = l = 0,\\
        \frac{\pi}{2}   & \text{if } k = l \neq 0.
    \end{cases}
\end{equation}
\end{lemma}

\begin{proof}
Recall that~$T_k(x) = \cos(k \arccos(x))$ for~$x \in [-1, 1]$. Let~$x = \cos \theta$ with $\theta \in [0, \pi]$. Then~$\dx = -\sin \theta \, d\theta$ and~$\sqrt{1 - x^2} = \sin \theta$.
Substituting, we have:
\begin{align*}
    \int\limits_{-1}^1 \frac{1}{\sqrt{1 - x^2}} T_k(x) T_l(x) \dx
    &= \int\limits_{\theta=\pi}^{0} \frac{1}{\sin \theta} \cos(k \theta) \cos(l \theta) (-\sin \theta) d\theta \\
    &= \int\limits_{0}^{\pi} \cos(k \theta) \cos(l \theta) d\theta
\end{align*}
The orthogonality of cosine functions on~$[0, \pi]$ gives:
\begin{align*}
    \int\limits_{0}^{\pi} \cos(k \theta) \cos(l \theta) d\theta =
    \begin{cases}
        0               & \text{if } k \neq l,\\
        \pi             & \text{if } k = l = 0,\\
        \frac{\pi}{2}   & \text{if } k = l \neq 0.
    \end{cases}
\end{align*}
This is a standard result in Fourier analysis~\cite{kreyszig} which proves the lemma.
\end{proof}

Lastly, we need a three-term recurrence relation for the Chebyshev polynomials. This is a well-known property of these polynomials, which can be derived using trigonometric identities.

\begin{lemma}[Three-term recurrence for Chebyshev polynomials]
For all integers $\mbox{$k \geq 2$}$, the Chebyshev polynomials of the first kind satisfy
\[
T_{k}(x) = 2xT_{k - 1}(x) - T_{k - 2}(x),
\]
where the starting conditions are
\[
T_0(x) = 1 \quad \text{and} \quad T_1(x) = x.
\]
\end{lemma}

\begin{proof}
Let $T_k(x) = \cos(k \arccos(x))$ for $x \in [-1, 1]$ and set $\theta = \arccos(x)$, so $\mbox{$T_k(x) = \cos(k\theta)$}$ and $x = \cos\theta$. The starting conditions are straightforward to verify, as $T_0(x) = \cos(0) = 1$ and $T_1(x) = \cos(\arccos(x)) = x$. For $k \geq 2$, we use the angle addition and subtraction formulas:
\begin{align*}
\cos(k\theta) &= \cos((k-1)\theta + \theta) = \cos((k-1)\theta)\cos\theta - \sin((k-1)\theta)\sin\theta \\
\cos((k-2)\theta) &= \cos((k-1)\theta - \theta) = \cos((k-1)\theta)\cos\theta + \sin((k-1)\theta)\sin\theta
\end{align*}
Adding these two equations gives:
\[
\cos(k\theta) + \cos((k-2)\theta) = 2\cos\theta \cos((k-1)\theta)
\]
which can be rearranged as
\[
\cos(k\theta) = 2\cos\theta \cos((k-1)\theta) - \cos((k-2)\theta)
\]
Substituting back $T_k(x) = \cos(k\theta)$ and $x = \cos\theta$, we obtain
\[
T_k(x) = 2x T_{k-1}(x) - T_{k-2}(x)
\]
as claimed.
\end{proof}

\section{Approximating the Spectral Density}
Define~$\hat{\phi}_h$ with the inverse of the weight function~$h$ as in \eqref{eq:phi_hat}, that is
\[
    \hat{\phi}_h(x) := h^{-1}(x) \, \phi(x) = \sqrt{1 - x^2} \; \frac{1}{n} \sum_{j = 1}^n \delta(x - \lambda_j).
\]
Again, as in \eqref{eq:phi_hat_polynomial_expansion}, we can express~$\hat{\phi}_h$ as an infinite series of Chebyshev polynomials:
\[
    \hat{\phi}_h(x) = \sum_{k = 0}^{\infty} \mu_k T_k(x)
\]
Now utilize the orthogonality \eqref{eq:orthogonality} of the Chebyshev polynomials to calculate a specific coefficient~$\mu_k$.
Note that~$\delta_{k0}$ in this context denotes the Kronecker delta, and therefore equals~$1$ for~$k = 0$ and~$0$ for~$k \neq 0$. Applying the inner product~\eqref{eq:inner_product} to both sides of the equation yields

\begin{align*}
    & \; \; \left \langle \sum_{l = 0}^{\infty} \mu_l T_l(x), T_k(x) \right \rangle_h = \left \langle \hat{\phi}(x), T_k(x) \right \rangle_h \\
    \implies & \int\limits_{-1}^1 \frac{1}{\sqrt{1 - x^2}} \left(\sum_{l = 0}^{\infty} \mu_l T_l(x)\right) T_k(x) \dx = \int\limits_{-1}^1 \frac{1}{\sqrt{1 - x^2}} \hat{\phi}(x) T_k(x) \dx.\\
\end{align*}On the left side, every term of the sum except for the~$k$-th term vanishes due to the orthogonality of the Chebyshev polynomials, while we make use of our definition of~$\hat{\phi}$ on the right side. Therefore, we have

\[
    \mu_k \frac{\pi}{2 - \delta_{k0}} = \int\limits_{-1}^1 \frac{1}{\sqrt{1 - x^2}} \sqrt{1 - x^2} \phi(x) T_k(x) \dx \implies \mu_k = \frac{2 - \delta_{k0}}{\pi} \int\limits_{-1}^1 \phi(x) T_k(x) \dx
\]
By applying the Dirac delta distribution we obtain:
\begin{align*}
    \mu_k = \frac{2 - \delta_{k0}}{\pi} \int\limits_{-1}^1 \phi(t) T_k(t) \dt &= \frac{2 - \delta_{k0}}{\pi} \int\limits_{-1}^1 \frac{1}{n} \sum_{j = 1}^n \delta(t - \lambda_j) T_k(t) \dt \\
    &= \frac{2 - \delta_{k0}}{n \pi} \sum_{j = 1}^n T_k(\lambda_j)\\
    &= \frac{2 - \delta_{k0}}{n \pi} \Tr(T_k(A))
\end{align*}
with $\Tr(T_k(A)) := \displaystyle \sum_{j = 1}^n T_k(\lambda_j)$ the \emph{trace} of the Chebyshev polynomial applied to the matrix $A$. This means we are in need of an estimator for $\Tr(T_k(A))$.

\section{Estimating the Trace of Chebyshev Polynomials}

We want to obtain the corollary of the following generalized theorem~\cite{hutchinson1989}:

\begin{theorem}
    Let $A \in \C^{n \times n}$ be a normal matrix with spectral decomposition
    \[
    A = U \Lambda U^* \quad \text{where} \quad UU^* = I_n \text{ and } \Lambda = \diag(\lambda_1, ..., \lambda_n)
    \]
    Let $\beta, v \in \C^n$ with $v = U\beta$.
    Suppose $v$ is a random vector whose entries $v_i$ are independent and identically distributed standard complex normal variables,
    i.e., $v_i \sim_\text{i.i.d.} \mathcal{N}_\C(0, 1)$, meaning $\Real(v_i), \Imag(v_i)$ are independent $\mathcal{N}(0, \frac{1}{2})$.
    Then
    \begin{equation} \label{eq:complex_normal_vector}
        \E[v] = 0 \quad \text{and} \quad \E[v v^*] = I_n,
    \end{equation}
    and it follows that
    \[
    \E[\beta] = 0 \quad \text{and} \quad \E[\beta \beta^*] = I_n.
    \]
\end{theorem}


\begin{proof}[Proof]
    Since the expectation operator is linear, it holds that
    \[
    \E[v] = \E[U\beta] = U\E[\beta] = 0 \implies \E[\beta] = 0
    \]
    Furthermore it holds that
    \[
    I_n = \E[vv^*] = \E[(U\beta)(U\beta)^*] = \E[U\beta \beta^*U^*] = U \E[\beta \beta^*]U^*
    \]
    Multiplying both sides with $U^*$ and $U$ yields:
    \[
    U^* I_n U = U^* U \E[\beta \beta^*]U^* U = \E[\beta \beta^*]
    \]
    Since $U$ is unitary, we have shown that $\E[\beta \beta^*] = I_n$.
\end{proof}

This theorem has a nice corollary when investigating a matrix function $f(A)$.
In that case,
\begin{align*}
    \E\left[v^* f(A) v\right] = \E\left[(U\beta)^* f(U\Lambda U^*) (U\beta)\right] &= \E\left[\beta^* U^* U f(\Lambda) U^* U \beta\right] \\
    &= \E\left[\beta^* f(\Lambda) \beta\right] \\
    &= \E\left[\sum_{j = 1}^n |\beta_j|^2 f(\lambda_j) \right] \\
    &= \sum_{j = 1}^n f(\lambda_j) \E\left[ |\beta_j|^2 \right] \\
    &= \sum_{j = 1}^n f(\lambda_j)
\end{align*}

or, more concisely,
\[
    \E\left[v^* f(A) v\right] = \Tr(f(A)).
\]

We now have a method to calculate the trace of a matrix function $f(A)$, using only vector multiplications with $A$. This is useful to estimate the trace of Chebyshev polynomials which we need to calculate our coefficients~$\mu_k$.

\section{The Kernel Polynomial Method Algorithm}

Now let $n_{\text{vec}} \in \N$ and consider random vectors $v_0^{(1)}, v_0^{(2)}, \dots, v_0^{(n_{\text{vec}})}$, each drawn independently from the standard normal distribution, that is, $\mathbb{E}[v_0^{(k)}] = 0$ and $\mathbb{E}\left[v_0^{(k)} \left(v_0^{(k)}\right)^*\right] = I_n$. The subscript $0$ indicates the vectors have not been multiplied by $A$. It follows from $\E\left[v^* f(A) v\right] = \Tr(f(A))$ that
\[
\zeta_k = \frac{1}{n_{\text{vec}}} \sum_{l = 1}^{n_{\text{vec}}} \left( v_0^{(l)} \right)^T T_k(A) v_0^{(l)}
\]
is a good estimator for $\Tr(T_k(A))$ and therefore
\[
\mu_k \approx \frac{2 - \delta_{k0}}{n \pi} \zeta_k.
\]

In order to determine the $\zeta_k$, let $v_0 \equiv v_0^{(l)}$.
Using the recursion formula for Chebyschev polynomials, we can calculate
\[
T_{k + 1}(A)v_0 = 2 A T_k(A) v_0 - T_{k - 1}(A) v_0.
\]
For $v_k \equiv T_k(A)v_0$ it also holds that
\[
v_{k + 1} = 2 A v_k - v_{k - 1}.
\]

With this we are fully equipped for the final calculation and the goal of the KPM is reached:
Instead of having to multiply matrices with other matrices, it now suffices to multiply matrices with vectors.
Now we can approximate $\phi(x)$ as closely as we like.
As aforementioned, it is not always desirable to have an infinitely exact approximation.
By the Riemann-Lebesgue lemma~\cite[p.~103]{rudin1987} it holds that
\[
\lim \limits_{k \to \infty} \mu_k \to 0
\]
and we are only interested in $T_k(x)$ with $k \leq M$.
Therefore we estimate $\phi$ with
\begin{equation} \label{Approximated spectral density}
    \tilde{\phi}_M(x) := \frac{1}{\sqrt{1 - x^2}} \sum_{k = 0}^{M} \mu_k T_k(x)
\end{equation}
Note that this approximation no longer satisfies the condition of non-negativity, as the Chebyshev polynomials oscillate between $-1$ and $1$. This will need to be considered when interpreting the results. Before we can state the final algorithm, we need to discuss how to apply the KPM to unitary matrices, as using the Cayley transform entails additional computations.

\section{The KPM for Unitary Matrices}
Applying the Cayley transform to an arbitrary unitary matrix~$U$ yields a Hermitian matrix~$H$, so the KPM can in principle be applied to unitary matrices as well. While explicitly forming $H = i(I_n - U)(I_n + U)^{-1}$ works for smaller matrices, it is highly impractical for larger ones: even if $U$ is sparse, the inverse $(I_n + U)^{-1}$ is typically dense, making $H$ expensive to store and destroying any sparsity. Instead, the Cayley transform is applied implicitly, as an operator acting on vectors. Whenever the algorithm requires the action of $H$ on a vector $v$, we compute
\[
    H v = (I_n - U)(I_n + U)^{-1} v
\]
by first solving the linear system
\begin{equation}\label{eq:linear_system}
    (I_n + U) w = v
\end{equation}
for $w$, and then evaluating $i(I_n - U)w$. This approach avoids explicit matrix inversion and leverages fast matrix-vector products and sparse linear solvers, preserving efficiency and scalability for large-scale problems. For large, sparse $U$, iterative methods such as GMRES or MINRES can be used to solve $(I_n + U)w = v$ efficiently, especially when a good preconditioner is available. This operator-based application is a key distinction from the Hermitian case, where simple matrix-vector products suffice.

We can even further reduce the number of matrix-vector products by the following observation: 
\[
(I_n + U) w = v \implies w + Uw = v \implies Uw = v - w.
\]
So evaluating $i(I_n - U)w$ can be simplified to $i(w - Uw) = i(w - (v - w)) = i(2w - v)$. This means only having to solve the linear system once and then only using vector operations to obtain the next vector in the recursion, never even having to multiply directly with a matrix.

Another consideration that needs to be made is regarding the eigenvalues of $U$. Because of our using Chebyshev polynomials for orthogonality, we established that a Hermitian matrix needs to have eigenvalues in the interval $[-1, 1]$. If we were starting with a Hermitian matrix, this would be but a simple transformation. How does this translate to unitary matrices?

Algorithm \ref{alg:cap} is based on \cite[p.~10]{linsaadyang14} and summarizes the steps described above.
The implementation is done in Python, and linked in the appendix.

\begin{algorithm}[H]
    \caption{The Kernel Polynomial Method}\label{alg:cap}
    \begin{algorithmic}[5]
    \Require $U \in \mathbb{C}^{n \times n}$ with $U^{-1} = U^*$, points $t_i$ to evaluate the spectral density, number of Chebyshev polynomials $M$, number of random vectors $n_{\text{vec}}$\\
    \Ensure Estimated spectral density \{$\tilde{\phi}_M(t_i)$\}\\
    \For{$k = 0 : M$}
    \State $\zeta_k \gets 0$
    \EndFor
    \For{$l = 1 : n_{\text{vec}}$}
    \State $\text{Choose a random new vector } v_0^{(l)}\text{;}$ \Comment{$v_{0_i}^{(l)} \sim_\text{ i.i.d. } \mathcal{N}(0, 1)$}
    \For{$k = 0 : M$}
    \State $\text{Calculate } \zeta_k \gets \zeta_k + \left( v_0^{(l)} \right)^T v_k{(l)}\text{;}$  
    \If{$k = 0$}
    \State $\text{Solve } (I_n + U) w_0^{(l)} = v_0^{(l)}$
    \State $v_1^{(l)} \gets i (2w_0^{(l)} - v_0^{(l)})$
    \Else
    \State $\text{Solve } (I_n + U) w_k^{(l)} = v_k^{(l)}$
    \State $v_{k+1}^{(l)} \gets 2 i (2w_k^{(l)} - v_k^{(l)}) - v_{k-1}^{(l)}$ \Comment{Three term recursion}
    \EndIf
    \EndFor
    \EndFor
    \For{$k = 0 : M$}
    \State $\zeta_k \gets \frac{\zeta_k}{n_{\text{vec}}}$
    \State $\mu_k \gets \frac{2 - \delta_{k0}}{n \pi} \zeta_k$
    \EndFor
    \State $\text{Evaluate } \tilde{\phi}_M(t_i) \eqref{Approximated spectral density}$
    \end{algorithmic}
\end{algorithm}

That concludes the description of the Kernel Polynomial Method. In the next chapter, we use this algorithm to generate some results, which we then compare with the actual spectral density. Furthermore, we discuss the choice of random unitary matrices, and how it influences the results.