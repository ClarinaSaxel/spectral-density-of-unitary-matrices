\section{Motivation}
Calculating the spectral density of a matrix is straightforward when its eigenvalues are already known.
However, this is rarely the case in practice, and computing the eigenvalues of very large matrices is both time-consuming and computationally expensive.
At the same time, the density of states (DOS), which serves as a probability density for the distribution of eigenvalues, is of great interest in many fields.
Therefore, there is a need for efficient methods to approximate the spectral density at low computational cost.

The challenge arises because $\phi(t)$, the delta distribution, is not a conventional \emph{function} that can be evaluated pointwise.\\
One intuitive approach is to select an interval $I \in \R$ such that the spectrum of $A$, $\sigma(A)$, is contained within $I$.
Next, choose $k$ points $t_i$ in $I$ so that the interval is divided into subintervals:
$$\{t_i\}_{i = 1}^k \subset I \quad \text{with} \quad \bigcup_{i = 1}^{k - 1} [t_i, t_{i+1}] = I$$
Count the number of eigenvalues in each subinterval.
Then, calculate the average value of $\phi(t)$ in each interval using $\nu_{[a, b]}$ from equation \ref{eq:nu_a_b}.
The result is a histogram, which, as the subintervals become smaller (i.e., as $k$ increases and $(t_{i+1} - t_i) \longrightarrow 0$), approaches the true spectral density.\\
To count the eigenvalues in the intervals, one can use methods such as Sylvester's law of inertia.
The details of this method are beyond the scope of this work;
it would require computing a decomposition of $A - t_i I = LDL^T$ for all $t_i$ \cite{golubvanloan}.
Instead, we prefer a method in which $A$ is multiplied by vectors, which is more efficient in higher dimensions.\\