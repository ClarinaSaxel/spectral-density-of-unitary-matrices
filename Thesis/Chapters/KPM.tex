\section{Overview}
The so called kernel polynomial method, or KPM for short,
has many variants and is a powerful tool for approximating the spectral density of matrices.
We will focus on the main approach in the following.\\
As the name suggests, the KPM is a polynomial extension of the spectral density.
The coefficients of the polynomials are derived from the method of moments,
in order to obtain an estimator function as in statistics.
The method is based on a corollary of the following theorem:


\begin{theorem}
    Let $A = A^T \in \R^{n \times n}$ with spectral decomposition
    \[
    A = U \Lambda U^T \quad \text{where} \quad UU^T = I_n \text{ and } \Lambda = \diag(\lambda_1, ..., \lambda_n)
    \] 
    Also, let $\beta, v \in \R^n$ with $v = U\beta$.\\
    If $v_i \sim_\text{i.i.d.} \mathcal{N}(0, 1)$ for the components $\{v_i\}_{i = 1}^n$ of $v$, that is to say
    \begin{equation} \label{eq:normalverteiltervektor}
        \E[v] = 0 \text{ and } \E[vv^T] = I_n \text{,}
    \end{equation}
    then
    \[
    \E[\beta] = 0 \text{ and } E[\beta \beta^T] = I_n
    \]
\end{theorem}


\begin{proof}[Proof of Theorem 1]
    It holds that
    \[
    \E[v] = \E[U\beta] = U\E[\beta] = 0 \implies \E[\beta] = 0
    \]
    Furthermore it holds that
    \[
    \E[vv^T] = \E[U\beta \beta^TU^T] = U \E[\beta \beta^T]U^T = U \E[\beta \beta^T]U^T = I_n
    \]
    Since $U$ is orthogonal, we can multiply both sides with $U^T$ and $U$:
    \[
    U^T \E[vv^T] U = \E[\beta \beta^T] = U^T I_n U = I_n
    \]
    Thus, we have shown that $\E[\beta \beta^T] = I_n$.
\end{proof}

\vspace{0.5 cm}
This theorem has a nice corollary when investigating a matrix function $f(A)$.
In that case, we have

\begin{align*}
    \E\left[v^Tf(A)v\right] = \E\left[(U\beta)^Tf(U\Lambda U^T)(U\beta)\right] & = \E\left[\beta^TU^TUf(\Lambda)U^TU\beta\right]\\
        & = \E\left[\beta^Tf(\Lambda)\beta\right]\\
        & = \E\left[\sum_{j = 1}^n \beta_j^2 f(\lambda_j) \right]\\
        & = \sum_{j = 1}^n f(\lambda_j) \E\left[ \beta_j^2 \right]\\
        & = \sum_{j = 1}^n f(\lambda_j)
\end{align*}

also zusammengefasst
\begin{equation} \label{eq:theoremresultat}
    \E\left[v^Tf(A)v\right] = \Spur(f(A))
\end{equation}

\section{Polynomiale Erweiterung durch Tschebyschev-Polynome}
Aufgrund ihrer vielen einzigartigen Eigenschaften sind Tschebyschev-Polynome besonders gut zur polynomialen Erweiterung der Delta-Distribution geeignet.
Mit Hilfe der trigonometrischen Funktionen können sie auch wie folgt ausgedrückt werden:
\[ T_k(t) =
\begin{cases}
    \cos(k \arccos(t))            & \quad \text{für } k \in [-1, 1]\\
    \cosh(k \arcosh(t))           & \quad \text{für } k > 1\\
    (-1)^k \cosh(k \arcosh(-t))   & \quad \text{für } k < -1
\end{cases}
\]

Wir benutzen im Folgenden nur die Formel $T_k(t) = \cos(k \arccos(t))$.
Daher müssen wir uns auf Matrizen beschränken, deren Eigenwerte im Intervall $[-1, 1]$ liegen.
Sollte diese Voraussetzung nicht erfüllt sein, kann man die Eigenwerte entsprechend transformieren.
Seien dazu $\lambda_{us}$ und $\lambda_{os}$ jeweils die untere bzw. obere Schranke für die Eigenwerte von $A$.
Definiere
$$c := \frac{\lambda_{us} + \lambda_{os}}{2} \quad \text{und} \quad d := \frac{\lambda_{os} - \lambda_{us}}{2}$$
Dann ist $B = \frac{A - c*\1_n}{d}$ eine Matrix mit Eigenwerten im Intervall $[-1, 1]$.
Eine Veranschaulichung dazu ist im Anhang verlinkt.

Tschebyschev-Polynome können zudem mit der Rekursionsformel
$$T_{k + 1}(t) = 2tT_k(t) - T_{k - 1}(t)$$
berechnet werden, wobei die Startbedingunen $T_0(t) = 1$ und $T_1(t) = x$ gelten.

Beachte auch, dass das Resultat in Gleichung \ref{eq:theoremresultat} besagt, dass
\begin{equation} \label{eq:Tschebyschev-Spur}
    \E\left[v^TT_k(A)v\right] = \sum_{j = 1}^n T_k(\lambda_j) = \Spur(T_k(A))
\end{equation}
gilt. Dies ist zentral im weiteren Vorgehen.\\

Sei nun
\begin{equation} \label{eq:Gewichtsfunktion}
    h(x) = \frac{1}{\sqrt{1 - t^2}}
\end{equation}
eine Gewichtsfunktion.
Eine weitere Eigenschaft der Tschebyschev-Polynome ist, dass sie \emph{orthogonal} bezüglich des mit $h$ gewichteten Skalarproduktes

$$\left \langle f, g \right \rangle = \int_{-1}^1 \frac{1}{\sqrt{1 - x^2}} \cdot f(x) \cdot g(x) \dx$$

sind. Das bedeutet, dass

$$\int_{-1}^1 \frac{1}{\sqrt{1 - t^2}} \cdot T_k(t) \cdot T_l(t) \dt =
\begin{cases}
    0               & \quad \text{für } k \neq l\\
    \pi             & \quad \text{für } k = l = 0\\
    \frac{\pi}{2}   & \quad \text{für } k = l \neq 0
\end{cases}$$

\section{Annäherung der Spektraldichte}
Multipliziere nun die Spektraldichte mit dem Inversen der Gewichtsfunktion \ref{eq:Gewichtsfunktion}:
$$\hat{\phi}(t) = \sqrt{1 - t^2} \phi(t) = \sqrt{1 - t^2} \times \frac{1}{n} \sum_{j = 1}^n \delta(t - \lambda_j)$$
Sei nun $g \in \SR$, dem in Definition \ref{def:Schwartz space} beschriebenen Schwartz-Raum,
und $\mu_k \in \R$ Koeffizienten,
die wir nachher berechnen, sodass die folgende Gleichung gilt:
\begin{equation} \label{eq:Distributionsgleichheit}
    \int \limits_{-1}^1 \hat{\phi}(t) g(t) \dt = \int \limits_{-1}^1 \sum_{k = 0}^{\infty} \mu_k T_k(t) g(t) \dt
\end{equation}
Gilt dies für beliebige $g \in \SR$, so vereinfachen wir Gleichung \ref{eq:Distributionsgleichheit} zu
\begin{equation} \label{eq:Tschebyschev-Erweiterung}
    \hat{\phi}(t) = \sum_{k = 0}^{\infty} \mu_k T_k(t)
\end{equation}
Nutze nun die Orthogonalität der Tschebyschev-Polynome aus, um einen bestimmten Koeffizienten $\mu_k$ zu berechnen:
\begin{align*}
    \sum_{l = 0}^{\infty} \mu_l T_l(t) = \hat{\phi}(t) & \implies \left(\sum_{l = 0}^{\infty} \mu_l T_l(t)\right) \cdot T_k(t) = \hat{\phi}(t) \cdot T_k(t)\\
    & \implies \int_{-1}^1 \frac{1}{\sqrt{1 - t^2}} \cdot \left(\sum_{l = 0}^{\infty} \mu_l T_l(t)\right) \cdot T_k(t) \dt = \int_{-1}^1 \frac{1}{\sqrt{1 - t^2}} \cdot \hat{\phi}(t) \cdot T_k(t) \dt\\
    & \implies \mu_k \cdot \frac{\pi}{2 - \delta_{k0}} = \int_{-1}^1 \frac{1}{\sqrt{1 - t^2}} \cdot \sqrt{1 - t^2} \cdot \phi(t) \cdot T_k(t) \dt\\
    & \implies \mu_k = \frac{2 - \delta_{k0}}{\pi} \cdot \int_{-1}^1 \phi(t) \cdot T_k(t) \dt\\
\end{align*}

Durch Anwendung der Delta-Funktion erhält man:
\begin{align*}
    \mu_k = \frac{2 - \delta_{k0}}{\pi} \cdot \int_{-1}^1 \phi(t) \cdot T_k(t) \dt &= \frac{2 - \delta_{k0}}{\pi} \cdot \int_{-1}^1 \frac{1}{n} \sum_{j = 1}^n \delta(t - \lambda_j) \cdot T_k(t) \dt \\
    &= \frac{2 - \delta_{k0}}{n \pi} \sum_{j = 1}^n T_k(\lambda_j)\\
    &= \frac{2 - \delta_{k0}}{n \pi} \Spur(T_k(A))
\end{align*}

Sei nun $n_{vec} \in \R$ und $v_0^{(1)}, v_0^{(2)}, \dots, v_0^{(n_{vec})}$ Vektoren, die die Bedingungen aus dem Theorem erfüllen,
also $\E[v_0^{(k)}] = 0$ und $\E\left[v_0^{(k)}\left(v_0^{(k)}\right)^T\right] = \1_n$.
Aus Gleichung \ref{eq:Tschebyschev-Spur} folgt, dass
$$\zeta_k = \frac{1}{n_{vec}} \sum_{l = 1}^{n_{vec}} \left( v_0^{(l)} \right)^T T_k(A) v_0^{(l)}$$
ein guter Schätzer für $\Spur(T_k(A))$ ist und damit
$$\mu_k \approx \frac{2 - \delta_{k0}}{n \pi} \zeta_k$$

Um die $\zeta_k$ zu bestimmen, sei im Folgenden $v_0 \equiv v_0^{(l)}$
Berechne nun mit Hilfe der Rekursionsformel für Tschebyschev-Polynome:
$$T_{k + 1}(A)v_0 = 2 A T_k(A) v_0 - T_{k - 1}(A) v_0$$
Für $v_k \equiv T_k(A)v_0$ gilt also, dass
$$v_{k + 1} = 2 A v_k - v_{k - 1}$$

Damit sind alle Bauteile zur Berechnung festgelegt und das Ziel der KPM erreicht:
Anstatt rechenaufwendig Matrizen mit anderen Matrizen zu multiplizieren, müssen wir sie nur noch mit Vektoren multiplizieren.
Nun können wir $\phi(t)$ beliebig nah annähren.
Wie bereits erwähnt, ist eine unendlich genaue Annäherung nicht immer wünschenswert.
Wegengilt
$$\lim \limits_{k \to \infty} \mu_k \to 0$$
und wir interessieren uns nur für $T_k(t)$ mit $k \leq M$\\
Daher schätzen wir $\phi$ durch
\begin{equation} \label{eq:Angenäherte Spektraldichte}
    \tilde{\phi}_M(t) = \frac{1}{\sqrt{1 - t^2}} \sum_{k = 0}^{M} \mu_k T_k(t)
\end{equation}
\newline
Der folgende Pseudocode basiert auf \cite[p.~10]{linsaadyang14} und fasst die oben beschriebenen Schritte zusammen.
Ich habe ihn selbst implementiert und im Anhang verlinkt.

\begin{algorithm}
    \caption{Die Kernel-Polynom-Methode}\label{alg:cap}
    \begin{algorithmic}[5]
    \Require $A = A^T \in \R^{n \times n}$ mit Eigenwerten aus dem Intervall $[-1, 1]$
    \Ensure Geschätzte Spektraldichte \{$\tilde{\phi}_M(t_i)$\}\\
    \For{$k = 0 : M$}
    \State $\zeta_k \gets 0$
    \EndFor
    \For{$l = 1 : n_{\text{vec}}$}
    \State $\text{Wähle einen neuen zufälligen Vektor } v_0^{(l)}\text{;}$ \Comment{$v_{0_i}^{(l)} \sim_\text{ i.i.d. } \mathcal{N}(0, 1)$}
    \For{$k = 0 : M$}
    \State $\text{Berechne } \zeta_k \gets \zeta_k + \left( v_0^{(l)} \right)^T v_k{(l)}\text{;}$  
    \If{$k = 0$}
    \State $v_1^{(l)} \gets A v_0^{(l)}$
    \Else
    \State $v_{k+1}^{(l)} \gets 2 A v_k^{(l)} - v_{k-1}^{(l)}$ \Comment{Drei-Term-Rekursion}
    \EndIf
    \EndFor
    \EndFor
    \For{$k = 0 : M$}
    \State $\zeta_k \gets \frac{\zeta_k}{n_{\text{vec}}}$
    \State $\mu_k \gets \frac{2 - \delta_{k0}}{n \pi} \zeta_k$
    \EndFor
    \State $\text{Werte } \tilde{\phi}_M(t_i) \text{ mit Gleichung } \ref{eq:Angenäherte Spektraldichte} \text{ aus} $
    \end{algorithmic}
\end{algorithm}