Because of its ability to capture the distribution of eigenvalues,
the spectral density of a matrix is a crucial concept in various fields.
For example, in quantum mechanics, it describes the density of energy states;
in statistics, it is related to the distribution of principal components;
and in machine learning, it provides insight into the behavior of large neural networks.
However, two main challenges arise when defining and computing the spectral density for a matrix $H$:

Firstly, the eigenvalues of $H$ are typically not known in advance.
If they were, computing the spectral density would be straightforward,
but this is rarely the case for large matrices.
Secondly, the spectral density is defined using the Dirac delta distribution,
which is not a conventional function and cannot be evaluated pointwise.
This makes direct computation infeasible,
especially for large matrices where eigenvalue decomposition is prohibitively expensive.

To address these challenges, a fundamental idea is to replace the delta distribution by a regular function $f$ with similar properties:
\begin{equation} \label{integral_equals_one}
    \int\limits_{\mathbb{R}} f(x)\, dx = 1,
\end{equation}

where $f(x)$ is very small or zero for most $x \neq 0$, and finite but large near $x = 0$. This regularization enables numerical approximation of the spectral density and is central to many practical algorithms.

For real symmetric matrices, a variety of efficient methods have been developed to approximate the spectral density,
often relying on this regularization principle.
However, many problems in mathematics and physics naturally lead to complex Hermitian or even unitary matrices,
for which the spectral density is less straightforward to compute.
In this thesis, we generalize the established approaches from the real symmetric case to the Hermitian case,
and further broaden the scope to unitary matrices.
This is achieved by first applying the Cayley transform, which provides a bridge between Hermitian and unitary matrices
and allows us to transfer techniques and insights between these classes.

One intuitive approach is to select an interval $I \subset \R$ containing the spectrum of $H$,
and divide it into $k$ subintervals using points $\{t_i\}_{i=1}^k$.
By counting the number of eigenvalues in each subinterval
and calculating the average value of $\phi(x)$ using $\nu_{[a, b]}$ from equation \ref{eq:nu_a_b},
one obtains a histogram that approximates the true spectral density as the subintervals become smaller.

To count the eigenvalues in the intervals, methods such as Sylvester's law of inertia can be used,
which require computing a decomposition of $A - t_i I = LDL^T$ for all $t_i$ \cite{golubvanloan}.
While this is efficient for certain structured matrices,
it becomes computationally expensive or impractical for large, unstructured matrices.

This motivates the development of efficient numerical methods that can approximate the spectral density
using only matrix-vector multiplications, which scale better with matrix size.
In this thesis, we explore such methods, focusing on regularization techniques and polynomial approximations
that make the computation of spectral densities feasible for large-scale problems.
These advances have the potential to impact a wide range of scientific and engineering disciplines
where understanding the spectral properties of large matrices is crucial.