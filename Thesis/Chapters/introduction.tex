The importance of eigenvalues in mathematics and physics is well established. For very large matrices, however, computing all eigenvalues can be prohibitively expensive. In many applications, it suffices to know only a few extremal eigenvalues or the overall distribution of eigenvalues, leading to the concept of the spectral density.

While the spectral density is well understood for real symmetric matrices, our goal is to extend this concept to the complex case. To achieve this, we first examine Hermitian matrices, which generalize symmetric matrices, introduce the Cayley transform as a bridge to unitary matrices, and finally define and investigate the spectral density for unitary matrices. With this foundation, we are well-equipped to proceed with the main algorithm.

\section{Unitary Matrices}

We start with two definitions for important real matrices that we then extend to complex matrices. The index $^T$ marks the transpose of a matrix. As common in literature, $I_n$ denotes the identity matrix of size $n$.

\begin{definition}[Symmetric matrix]
    Let $A$ be a real, square matrix of size $n$.
    Then $A$ is called \emph{symmetric} if $A^T = A$.
\end{definition}

The following definition is for matrices with their transpose as their inverse.

\begin{definition}[Orthogonal matrix]
    Let $A$ be a real, square matrix of size $n$.
    Then $A$ is called \emph{orthogonal} if $A^T \cdot A = A \cdot A^T = I_n$.
\end{definition}

Now we define the complex equivalent of a real symmetric matrix. Here, $A^*$ denotes the \emph{conjugate transpose} (also called adjoint) of $A$, that is, the matrix obtained by taking the transpose and then complex conjugating all entries.

\begin{definition}[Hermitian matrix]
    Let $A$ be a complex square matrix of size $n$.
    Then $A$ is called \emph{Hermitian} if $A^* = A$.
\end{definition}

Throughout this thesis, $A$ will denote a complex, square matrix of size $n$, unless stated otherwise. When referring to a Hermitian matrix, we will use the letter $H$.

We now examine the eigenvalues of Hermitian matrices.

Let $H = H^*$ and $H v = \lambda v$ for a complex vector $v \neq \mathbf{0}$ of size $n$ and a scalar $\lambda \in \C$. Consider now the inner product $ v^* v$. Remember that $(A \cdot B)^* = B^* \cdot A^*$ holds for a matrix product, and obviously ${(A^*)^*} = A$ for all matrices and vectors.

\[
    \lambda v^* v = v^* \left( \lambda v \right)
    = v^* \left( H v \right)
    = \left(v^* H \right) v
    = \left( H^* v \right)^* v
    = \left( H v \right)^* v
    = (\lambda v)^* v
    = \overline{\lambda} v^* v
\]

Since we have that $v \neq \mathbf{0}$ it follows that $v^* v \neq \mathbf{0}$ and therefore $\lambda = \overline{\lambda}$, that is to say $\lambda$ is real. This means that all eigenvalues of Hermitian matrices are real numbers. It follows that all eigenvalues of symmetric matrices are also real numbers, since they are a special case of Hermitian matrices. Now we can define the complex equivalent of orthogonal matrices:

\begin{definition}[Unitary matrix]
    A matrix $A$ is called \emph{unitary} if $A^* \cdot A = A \cdot A^* = I_n$.
\end{definition}

We will oftentimes denote unitary matrices by using $U$ as a reference. It is easy to see that orthogonal matrices are a special case of unitary matrices, since $A^T = A^*$ for all real matrices.

Consider a unitary matrix $U$ and an eigenpair $(\lambda, v)$ of $U$. The complex conjugate of the eigenvalue equation $U v = \lambda v$ is

\[
    v^* U^* = v^* \overline{\lambda} = \overline{\lambda} v^*
\]

We calculate

\[
    v^* v = v^* I_n v = v^* U^* U v = v^* \overline{\lambda} \lambda v = \overline{\lambda} \lambda v^* v = \left| \lambda \right|^2 v^* v
\]

Similarly to above, we can divide by $v^* v$ to obtain

\[
    1 = \left| \lambda \right|^2 \implies \left| \lambda \right| = 1
\]

meaning that all eigenvalues of unitary matrices have a length of $1$ and are thus situated on the unit circle. As they are a special case of unitary matrices, the same goes for orthogonal matrices. This property is crucial, as it enables the application of the Cayley transform, introduced in the following section.

There is a special group of matrices that all the matrices we have defined so far belong to.

\begin{definition}[Normal matrix]
    A matrix $A$ is called \emph{normal} if it commutes with its conjugate transpose,
    that is to say $A^* A = A A^*$.
\end{definition}

It is straightforward to see that both Hermitian and unitary matrices are normal matrices and that the notion includes real symmetric and orthogonal matrices as special cases. The spectral theorem states that normal matrices can be diagonalized by a unitary matrix \cite{sheldonaxler}. That means, for any normal matrix $A$, there exists a unitary matrix $U$ such that $A = U \Lambda U^*$, where $\Lambda = \diag(\lambda_1, \ldots, \lambda_n)$ with $\lambda_1, \ldots, \lambda_n$ being the eigenvalues of $A$. This is essential for applying a function to a matrix, which we will get to in the next section.

\section{Cayley Transform}

Before giving the central definition of this section, we will first extend the concept of functions to normal matrices, allowing us to map one matrix to another.

\begin{definition}[Matrix function on normal matrices]
    Let $A$ be a normal matrix and let $f: \C \to \C$ be a function that is defined on the spectrum of $A$,
    $\sigma(A) = \{\lambda_1, \ldots, \lambda_n\}$.
    Then the \emph{matrix function} $f(A)$ is defined as
    \[
    f(A) := U f(\Lambda) U^* = U \diag(f(\lambda_1), \ldots, f(\lambda_n)) U^*
    \]
    where $U$ is the matrix of eigenvectors of $A$ and $\Lambda = \diag(\lambda_1, \ldots, \lambda_n)$ is the diagonal matrix of eigenvalues.
\end{definition}

Now we can define the \emph{Cayley transform}, which is a specific matrix function, that establishes a correspondence between Hermitian and unitary matrices, allowing spectral properties to be translated between these two important classes.

For a complex number $z \in \mathbb{C}$ with $z \neq -i$, the Cayley transform is defined as
\[
\varphi(z) = \frac{i - z}{i + z}.
\]

This function maps the real line to the unit circle in the complex plane.

\vspace{0.5cm}

\input{Graphics/cayley_transform.tex}

For matrices, the Cayley transform maps a Hermitian matrix $H$ (with $i + H$ invertible) to a unitary matrix $U$ via
\[
U = (i - H)(i + H)^{-1}.
\]
The condition of $i + H$ being invertible is the same as requiring that $H$ does not have $-i$ as an eigenvalue. If (and only if) that were the case, then we had $H v = -i v$ for some eigenvector $v \neq \mathbf{0}$, and therefore
\[
(i + H) v = i v + H v = i v + (-i v) = 0.
\]

Since Hermitian matrices have only real eigenvalues as discussed above, $-i$ can never be an eigenvalue. Conversely, given a unitary matrix $U$ (with $U \neq -I_n$), the inverse Cayley transform yields a Hermitian matrix:
\[
H = i (I_n - U)(I_n + U)^{-1}.
\]

This will be relevant later, as we can then use $\varphi$ to transform unitary matrices into symmetric ones and vice versa.

\section{Spectral Density}

To get to the notion of the spectral density, we will first need some more basic definitions to build on. The reader is assumed to be familiar with the concepts of distributions. Let us also recap that for $\Omega \subset \C^n$ open and non-empty, a \emph{test function} is a smooth function with compact support defined on $\Omega$. The space of all test functions on $\Omega$ is usually denoted by $\mathcal{E}$.

We will now look at an important case of a distribution

\begin{definition}[Dirac delta distribution]
    Let $\mathcal{E} = \Cinfty(\Omega)$ with $0 \in \Omega \subset \R^n$.
    Then
    \[
    \delta: \mathcal{E} \to \R, \quad f \mapsto f(0) \quad \text{with} \quad \delta(f) = \langle \delta, f \rangle = f(0)
    \]
\end{definition}

this distribution is often mistakenly referred to as a function, although it is not a function in the classical sense.

The Dirac delta is characterized by the following property:

\[
\int\limits_{-\infty}^{\infty} f(x) \delta(x-a) \dx = \int\limits_{-\infty}^{\infty} f(x) \delta(a-x) \dx = f(a) \implies \int\limits_{-\infty}^{\infty} \delta(x-a) \dx = 1.
\]

This means that the Dirac delta distribution is zero everywhere except at the point $a$, where it is infinitely high, such that the integral over it equals $1$. We now have all the tools we need to define the central concept of this thesis:

\begin{definition}[Spectral density]
    Let $H$ be hermitian and sparse.
    For $x \in \R$, the \emph{spectral density} is then defined as
    \[
    \phi(x) = \frac{1}{n} \sum_{j=1}^{n} \delta(x - \lambda_j)
    \]
    where $\delta$ is the Dirac delta distribution
    and $\lambda_j$ are the eigenvalues of $H$ in non-descending order.
\end{definition}

The summation over the $n$ eigenvalues of $H$ creates a spike at each eigenvalue $\lambda_j$, resulting in a distribution that is zero everywhere except at the eigenvalues of $H$. The division by $n$ ensures that the spectral density is normalized, meaning that the integral over the entire real line equals $1$. This is an important property, as it allows us to interpret the spectral density as a probability density function.

The number of eigenvalues $\nu$ in an interval $[a, b]$ can then be counted in the following manner:

\begin{equation} \label{eq:nu_a_b}
    \nu_{[a, b]} = \int\limits_a^b \sum_j \delta(t - \lambda_j) \dt \equiv \int\limits_a^b n \phi(t) \dt
\end{equation}

Lastly, we will need the notion of the Schwartz space, which we will use as the space of test functions in the following sections.

\begin{definition}[Schwartz space over $\R$] \label{def:Schwartz space}
    The \emph{Schwartz space} over $\R$ consists of all smooth functions $f$ that decay rapidly to zero as $|x|$ approaches infinity \cite{richtmyer}.
    Formally,
    \[
    \SR := \left\{f \in \Cinfty(\R) \mid \forall p, k \in \N_0: \sup_{x \in \R} \left| x^p f^{(k)}(x) \right| < \infty \right\}
    \]
\end{definition}