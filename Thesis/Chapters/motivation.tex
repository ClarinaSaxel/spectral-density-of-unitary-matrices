Because of its ability to capture the distribution of eigenvalues,
the spectral density of a matrix is a crucial concept in various fields, including physics, statistics,
and machine learning, although this list is not exhaustive.
However, two challenges arise when defining the spectral density of a matrix $H$:

Firstly, the eigenvalues of the matrix $H$ are typically not known in advance.
This would make the computation of the spectral density straightforward, but also defeat the purpose.

Secondly, the delta distribution $\phi(x)$ is not a conventional \emph{function} that can be evaluated pointwise.
Therefore, there is a need for efficient methods to approximate the spectral density at low computational cost.

A fundamental idea to address this is to replace the delta distribution by a regular function $f$ with similar properties:
\[
\int\limits_{\mathbb{R}} f(x)\, dx = 1,
\]
where $f(x)$ is very small or zero for most $x \neq 0$, and finite but large near $x = 0$.
This regularization makes it possible to approximate the spectral density numerically and is central to many practical algorithms.

One intuitive approach is to select an interval $I \subset \R$ such that the spectrum of $H$, $\sigma(H)$, is contained within $I$.
Next, choose $k$ points $t_i$ in $I$ such that the interval is divided into subintervals:

\[
\{t_i\}_{i = 1}^k \subset I \quad \text{with} \quad \bigcup_{i = 1}^{k - 1} [t_i, t_{i+1}] = I
\]

Count the number of eigenvalues in each subinterval.
Then, calculate the average value of $\phi(x)$ in each interval using $\nu_{[a, b]}$ from equation \ref{eq:nu_a_b}.
The result is a histogram, which, as the subintervals become smaller (i.e., as $k$ increases and $(t_{i+1} - t_i) \longrightarrow 0$), approaches the true spectral density.

To count the eigenvalues in the intervals, one can use methods such as Sylvester's law of inertia.
The details of this method are beyond the scope of this work;
it would require computing a decomposition of $A - t_i I = LDL^T$ for all $t_i$ \cite{golubvanloan}.
While this can be computationally expensive for general matrices,
it remains a viable and efficient option for certain structured matrices.
However, for large and unstructured matrices, we prefer methods based on matrix-vector multiplications,
which are more efficient in higher dimensions.